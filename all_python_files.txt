# Contents of ./utils/__init__.py


# Contents of ./utils/resource_tracker.py
import resource
import time

class ResourceTracker:
    def __init__(self, ignore_time_limits: bool = False):
        self.start_time = None
        self.start_memory = None
        self.ignore_time_limits = ignore_time_limits

    def start(self):
        self.start_time = time.time()
        self.start_memory = resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss

    def stop(self):
        end_time = time.time()
        end_memory = resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss
        time_taken = end_time - self.start_time
        memory_used = (end_memory - self.start_memory) if end_memory >= self.start_memory else end_memory
        return time_taken, memory_used

    def set_limits(self, time_limit: int, memory_limit: int):
        if not self.ignore_time_limits:
            resource.setrlimit(resource.RLIMIT_CPU, (time_limit, time_limit))
            resource.setrlimit(resource.RLIMIT_AS, (memory_limit * 1024 * 1024, memory_limit * 1024 * 1024))


# Contents of ./utils/logger.py
import logging
import json
import os

class Logger:
    def __init__(self, name="JudgeLogger", level=logging.INFO):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(level)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)

    def log(self, level, message):
        if level.lower() == 'info':
            self.logger.info(message)
        elif level.lower() == 'debug':
            self.logger.debug(message)
        elif level.lower() == 'warning':
            self.logger.warning(message)
        elif level.lower() == 'error':
            self.logger.error(message)
        elif level.lower() == 'critical':
            self.logger.critical(message)

class JSONLogger:
    def __init__(self, filename):
        self.filename = filename
        if os.path.exists(self.filename):
            with open(self.filename, 'r') as file:
                self.data = json.load(file)
        else:
            self.data = {
                "problems": [],
                "total_passed_problems": 0
            }
        os.makedirs(os.path.dirname(self.filename), exist_ok=True)

    def log_initial_config(self, config):
        self.data["initial_config"] = {
            "ignore_time_limits": config.ignore_time_limits,
            "model": config.model,
            "provider": config.provider,
            "categories": config.categories,
            "shots": config.shots
        }
        self._write_log()

    def log_problem(self, title, category, results, solution, total_problems_passed, shots_info):
        passed_count = sum(1 for result in results if result['pass'])
        total_count = len(results)
        exceeded_time_count = sum(1 for result in results if "Time limit exceeded" in result.get("error", ""))
        exceeded_memory_count = sum(1 for result in results if "Memory limit exceeded" in result.get("error", ""))

        mean_time_taken = sum(result['time_taken'] for result in results) / total_count if total_count > 0 else 0
        mean_memory_used = sum(result['memory_used'] for result in results) / total_count if total_count > 0 else 0

        problem_passed = passed_count == total_count

        problem_log = {
            "title": title,
            "category": category,
            "solution": solution,
            "passed_test_cases": passed_count,
            "total_test_cases": total_count,
            "mean_time_taken": mean_time_taken,
            "mean_memory_used": mean_memory_used,
            "failure_reasons": [result["log"] for result in results if not result["pass"]],
            "failed_test_cases": [i + 1 for i, result in enumerate(results) if not result["pass"]],
            "exceeded_time_count": exceeded_time_count,
            "exceeded_memory_count": exceeded_memory_count,
            "total_problems_passed": total_problems_passed,
            "passed": problem_passed,
            "shots_info": shots_info
        }
        self.data["problems"].append(problem_log)
        if problem_passed:
            self.data["total_passed_problems"] += 1
        self._write_log()

    def log_compilation_error(self, title, category, solution, error_message, total_problems_passed, shot):
        problem_log = {
            "title": title,
            "category": category,
            "solution": solution,
            "passed_test_cases": 0,
            "total_test_cases": 0,
            "mean_time_taken": 0,
            "mean_memory_used": 0,
            "failure_reasons": [error_message],
            "failed_test_cases": [],
            "exceeded_time_count": 0,
            "exceeded_memory_count": 0,
            "total_problems_passed": total_problems_passed,
            "passed": False,
            "shot": shot
        }
        self.data["problems"].append(problem_log)
        self._write_log()

    def _write_log(self):
        with open(self.filename, 'w') as file:
            json.dump(self.data, file, indent=4)


# Contents of ./utils/models.py
from pydantic import BaseModel, validator
from typing import List, Optional

class Example(BaseModel):
    input: str
    output: str

class TestCase(BaseModel):
    input: str
    output: str

class Problem(BaseModel):
    title: str
    time_limit: str
    memory_limit: str
    problem_statement: str
    input: str
    output: str
    constraints: str
    example: Example
    test_cases: List[TestCase]
    category: Optional[str] = None

    @validator('time_limit')
    def parse_time_limit(cls, v):
        if v.endswith(" s"):
            return int(float(v.replace(" s", "")))
        raise ValueError("Invalid time limit format")

    @validator('memory_limit')
    def parse_memory_limit(cls, v):
        if v.endswith(" MB"):
            return int(v.replace(" MB", ""))
        raise ValueError("Invalid memory limit format")

class Config(BaseModel):
    ignore_time_limits: bool
    provider: str
    api_key: str
    model: str
    base_prompt: str = "You are a helpful assistant."
    categories: Optional[List[str]] = None
    shots: int = 1
    continue_from_log: Optional[str] = None 
    language: str = "cpp"


# Contents of ./utils/utils.py
import re

def sanitize_filename(name: str) -> str:
    sanitized = re.sub(r'[^a-zA-Z0-9_-]', '', name)
    return sanitized


# Contents of ./providers/base.py
from abc import ABC, abstractmethod
from utils.logger import Logger
import re

class BaseProvider(ABC):
    def __init__(self, logger: Logger, language: str):
        self.logger = logger
        self.language = language

    @abstractmethod
    def generate_solution(self, problem_statement: str) -> str:
        pass

    def extract_code(self, raw_solution: str) -> str:
        code_block = f"```{self.language}"
        code_match = re.search(rf"{code_block}(.*?```)", raw_solution, re.DOTALL)
        if code_match:
            code = code_match.group(1).strip().strip("`")
            self.logger.log('info', f"Extracted {self.language} solution: {code}")
            return code
        else:
            self.logger.log('error', f"Failed to extract {self.language} code from the solution")
            return ""


# Contents of ./providers/huggingface.py
from transformers import pipeline
from providers.base import BaseProvider
from utils.logger import Logger

#EXPERIMENTAL
class HuggingFaceProvider(BaseProvider):
    def __init__(self, model: str, base_prompt: str, logger: Logger, language: str):
        super().__init__(logger, language)
        self.model = model
        self.base_prompt = base_prompt
        self.generator = pipeline('text-generation', model=model)

    def generate_solution(self, problem: dict) -> str:
        self.logger.log('info', f"Generating solution using HuggingFace model {self.model}")

        prompt = (
            f"{self.base_prompt}\n"
            f"Problem Title: {problem['title']}\n"
            f"Problem Statement: {problem['problem_statement']}\n"
            f"Input: {problem['input']}\n"
            f"Output: {problem['output']}\n"
            f"Constraints: {problem['constraints']}\n"
            f"Example Input: {problem['example']['input']}\n"
            f"Example Output: {problem['example']['output']}\n"
            f"\nProvide the solution in a markdown {self.language} block.\n"
        )

        response = self.generator(prompt, max_length=1500, num_return_sequences=1)
        raw_solution = response[0]['generated_text'].strip()
        self.logger.log('info', f"Generated raw solution: {raw_solution}")

        return self.extract_code(raw_solution)


# Contents of ./providers/mistral.py
from mistralai.client import MistralClient
from mistralai.models.chat_completion import ChatMessage
from providers.base import BaseProvider
from utils.logger import Logger

class MistralProvider(BaseProvider):
    def __init__(self, api_key: str, model: str, base_prompt: str, logger: Logger, language: str):
        super().__init__(logger, language)
        self.client = MistralClient(api_key=api_key)
        self.model = model
        self.base_prompt = base_prompt

    def generate_solution(self, problem: dict) -> str:
        self.logger.log('info', f"Generating solution using Mistral model {self.model}")

        prompt = (
            f"{self.base_prompt}\n"
            f"Problem Title: {problem['title']}\n"
            f"Problem Statement: {problem['problem_statement']}\n"
            f"Input: {problem['input']}\n"
            f"Output: {problem['output']}\n"
            f"Constraints: {problem['constraints']}\n"
            f"Example Input: {problem['example']['input']}\n"
            f"Example Output: {problem['example']['output']}\n"
            f"\nProvide the solution in a markdown {self.language} block.\n"
        )

        messages = [
            ChatMessage(role="user", content=prompt)
        ]

        response = self.client.chat(
            model=self.model,
            messages=messages
        )
        
        raw_solution = response.choices[0].message.content.strip()
        self.logger.log('info', f"Generated raw solution: {raw_solution}")

        return self.extract_code(raw_solution)


# Contents of ./providers/openai.py
import openai
from providers.base import BaseProvider
from utils.logger import Logger

class OpenAIProvider(BaseProvider):
    def __init__(self, api_key: str, model: str, base_prompt: str, logger: Logger, language: str):
        super().__init__(logger, language)
        openai.api_key = api_key
        self.model = model
        self.base_prompt = base_prompt

    def generate_solution(self, problem: dict) -> str:
        self.logger.log('info', f"Generating solution using OpenAI model {self.model}")

        prompt = (
            f"{self.base_prompt}\n"
            f"Problem Title: {problem['title']}\n"
            f"Problem Statement: {problem['problem_statement']}\n"
            f"Input: {problem['input']}\n"
            f"Output: {problem['output']}\n"
            f"Constraints: {problem['constraints']}\n"
            f"Example Input: {problem['example']['input']}\n"
            f"Example Output: {problem['example']['output']}\n"
            f"\nProvide the solution in a markdown {self.language} block.\n"
        )

        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
        
        response = openai.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=0.7,
            max_tokens=1500,
            top_p=1.0,
            frequency_penalty=0.0,
            presence_penalty=0.0,
        )
        raw_solution = response.choices[0].message.content.strip()
        self.logger.log('info', f"Generated raw solution: {raw_solution}")

        return self.extract_code(raw_solution)


# Contents of ./providers/__int__.py


# Contents of ./providers/anthropic.py
import anthropic
from providers.base import BaseProvider
from utils.logger import Logger

# EXPERIMENTAL
class AnthropicProvider(BaseProvider):
    def __init__(self, api_key: str, model: str, base_prompt: str, logger: Logger, language: str):
        super().__init__(logger, language)
        self.api_key = api_key
        self.model = model
        self.base_prompt = base_prompt
        self.client = anthropic.Anthropic(api_key=api_key)

    def generate_solution(self, problem: dict) -> str:
        self.logger.log('info', f"Generating solution using Anthropic model {self.model}")

        prompt = (
            f"{self.base_prompt}\n"
            f"Problem Title: {problem['title']}\n"
            f"Problem Statement: {problem['problem_statement']}\n"
            f"Input: {problem['input']}\n"
            f"Output: {problem['output']}\n"
            f"Constraints: {problem['constraints']}\n"
            f"Example Input: {problem['example']['input']}\n"
            f"Example Output: {problem['example']['output']}\n"
            f"\nProvide the solution in a markdown {self.language} block.\n"
        )

        response = self.client.messages.create(
            model=self.model,
            max_tokens=1500,
            temperature=0.7,
            system="You are a helpful assistant.",
            messages=[{"role": "user", "content": prompt}]
        )
        raw_solution = response['completion'].strip()
        self.logger.log('info', f"Generated raw solution: {raw_solution}")

        return self.extract_code(raw_solution)


# Contents of ./judges/__init__.py


# Contents of ./judges/base.py
from abc import ABC, abstractmethod
from utils.logger import Logger
from utils.models import Problem
from utils.resource_tracker import ResourceTracker
from typing import List, Dict
import subprocess
import resource

class BaseJudge(ABC):
    def __init__(self, logger: Logger, language_extension: str):
        self.logger = logger
        self.language_extension = language_extension

    @abstractmethod
    def compile_code(self, source_path: str, output_binary: str) -> bool:
        pass

    @abstractmethod
    def run_code(self, binary_path: str, input_data: str, time_limit: int, memory_limit: int, ignore_time_limits: bool) -> Dict:
        pass

    def validate_output(self, actual_output: str, expected_output: str) -> bool:
        return actual_output.strip() == expected_output.strip()

    def judge_problem(self, problem: Problem, binary_path: str, ignore_time_limits: bool) -> List[Dict]:
        results = []
        for test_case in problem.test_cases:
            result = self.run_code(binary_path, test_case.input, problem.time_limit, problem.memory_limit, ignore_time_limits)
            pass_fail = self.validate_output(result["output"], test_case.output)
            results.append({
                "input": test_case.input,
                "expected_output": test_case.output,
                "actual_output": result["output"],
                "pass": pass_fail,
                "log": result["error"],
                "time_taken": result["time_taken"],
                "memory_used": result["memory_used"]
            })
        return results

    def _set_limits(self, time_limit: int, memory_limit: int, ignore_time_limits: bool):
        if not ignore_time_limits:
            resource.setrlimit(resource.RLIMIT_CPU, (time_limit, time_limit))
            resource.setrlimit(resource.RLIMIT_AS, (memory_limit * 1024 * 1024, memory_limit * 1024 * 1024))


# Contents of ./judges/cpp_judge.py
import subprocess
from typing import List, Dict
from utils.logger import Logger
from utils.models import Problem
from judges.base import BaseJudge
from utils.resource_tracker import ResourceTracker

class CppJudge(BaseJudge):
    def __init__(self, logger: Logger):
        super().__init__(logger, "cpp")

    def compile_code(self, source_path: str, output_binary: str) -> bool:
        self.logger.log('info', f"Compiling {source_path} to {output_binary}")
        compile_command = ["g++", source_path, "-o", output_binary]
        result = subprocess.run(compile_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        if result.returncode == 0:
            self.logger.log('info', "Compilation successful")
            return True
        else:
            self.logger.log('error', f"Compilation failed: {result.stderr.decode()}")
            return False

    def run_code(self, binary_path: str, input_data: str, time_limit: int, memory_limit: int, ignore_time_limits: bool) -> Dict:
        tracker = ResourceTracker(ignore_time_limits)
        tracker.start()

        try:
            result = subprocess.run([binary_path], input=input_data, text=True,
                                    stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                                    preexec_fn=lambda: tracker.set_limits(time_limit, memory_limit) if not ignore_time_limits else None, 
                                    timeout=None if ignore_time_limits else time_limit)
            time_taken, memory_used = tracker.stop()
            return {
                "success": True,
                "output": result.stdout,
                "error": result.stderr,
                "time_taken": time_taken,
                "memory_used": memory_used
            }
        except subprocess.TimeoutExpired:
            return {"success": False, "output": "", "error": "Time limit exceeded", "time_taken": time_limit, "memory_used": memory_used}
        except MemoryError:
            return {"success": False, "output": "", "error": "Memory limit exceeded", "time_taken": 0, "memory_used": memory_limit}
        except FileNotFoundError as e:
            return {"success": False, "output": "", "error": f"File not found: {e}", "time_taken": 0, "memory_used": 0}


# Contents of ./judges/python_judge.py
from typing import List, Dict
from utils.logger import Logger
from utils.models import Problem
from judges.base import BaseJudge
from utils.resource_tracker import ResourceTracker
import subprocess

class PythonJudge(BaseJudge):
    def __init__(self, logger: Logger):
        super().__init__(logger, "py")

    def compile_code(self, source_path: str, output_binary: str) -> bool:
        return True

    def run_code(self, binary_path: str, input_data: str, time_limit: int, memory_limit: int, ignore_time_limits: bool) -> Dict:
        tracker = ResourceTracker(ignore_time_limits)
        tracker.start()

        try:
            result = subprocess.run(['python', binary_path], input=input_data, text=True,
                                    stdout=subprocess.PIPE, stderr=subprocess.PIPE, 
                                    preexec_fn=lambda: tracker.set_limits(time_limit, memory_limit) if not ignore_time_limits else None, 
                                    timeout=None if ignore_time_limits else time_limit)
            time_taken, memory_used = tracker.stop()
            return {
                "success": True,
                "output": result.stdout,
                "error": result.stderr,
                "time_taken": time_taken,
                "memory_used": memory_used
            }
        except subprocess.TimeoutExpired:
            return {"success": False, "output": "", "error": "Time limit exceeded", "time_taken": time_limit, "memory_used": memory_used}


# Contents of ./main.py
import os
import json
from tqdm import tqdm
from pydantic import ValidationError
from datasets import load_dataset
from judges.cpp_judge import CppJudge
from judges.python_judge import PythonJudge
from utils.logger import Logger, JSONLogger
from utils.models import Problem, Config
from utils.utils import sanitize_filename
from providers.openai import OpenAIProvider
from providers.huggingface import HuggingFaceProvider
from providers.anthropic import AnthropicProvider
from providers.mistral import MistralProvider

def load_problems_from_hf(dataset_name: str, split: str = 'train') -> list[str]:
    dataset = load_dataset(dataset_name, split=split)
    return [json.dumps(problem) for problem in dataset]

def load_config(config_path: str) -> Config:
    with open(config_path, 'r') as file:
        config_json = json.load(file)
    return Config(**config_json)

def generate_summary(results: list[dict]) -> str:
    passed_count = sum(1 for result in results if result['pass'])
    total_count = len(results)
    return f"Passed {passed_count}/{total_count} test cases"

def load_existing_log(log_filename: str) -> dict:
    if os.path.exists(log_filename):
        with open(log_filename, 'r') as file:
            return json.load(file)
    return {}

def initialize_provider(config: Config, logger: Logger):
    if config.provider == "openai":
        return OpenAIProvider(config.api_key, config.model, config.base_prompt, logger, config.language)
    elif config.provider == "huggingface":
        return HuggingFaceProvider(config.model, config.base_prompt, logger, config.language)
    elif config.provider == "anthropic":
        return AnthropicProvider(config.api_key, config.model, config.base_prompt, logger, config.language)
    elif config.provider == "mistral":
        return MistralProvider(config.api_key, config.model, config.base_prompt, logger, config.language)
    else:
        logger.log('error', "Invalid provider specified")
        raise ValueError("Invalid provider specified")

def process_problem(judge, provider, problem_data: dict, shots: int, ignore_time_limits: bool, json_logger: JSONLogger, logger: Logger, problems_passed: int, total_filtered_problems: int, index: int) -> int:
    problem_title = problem_data['title']
    sanitized_title = sanitize_filename(problem_title)
    
    for shot in range(1, shots + 1):
        source_file = os.path.join("temp", f"{sanitized_title}_shot_{shot}.{judge.language_extension}")
        binary_file = os.path.join("temp", f"{sanitized_title}_binary_shot_{shot}")

        solution = provider.generate_solution(problem_data)
        if solution:
            with open(source_file, 'w') as file:
                file.write(solution)
            
            if judge.compile_code(source_file, binary_file):
                try:
                    problem = Problem(**problem_data)
                    results = judge.judge_problem(problem, binary_file, ignore_time_limits)
                    summary = generate_summary(results)
                    logger.log('info', f"Problem {index + 1}/{total_filtered_problems} Shot {shot}: {summary}")
                    if all(result['pass'] for result in results):
                        problems_passed += 1
                        json_logger.log_problem(problem.title, problem.category or "Uncategorized", results, solution, problems_passed, {"shot": shot, "status": "passed"})
                        break
                    else:
                        json_logger.log_problem(problem.title, problem.category or "Uncategorized", results, solution, problems_passed, {"shot": shot, "status": "failed"})
                except ValidationError as e:
                    logger.log('error', f"Problem validation error: {e}")
            else:
                logger.log('error', "Compilation failed")
                json_logger.log_compilation_error(problem_data["title"], problem_data.get("category", "Uncategorized"), solution, "Compilation failed", problems_passed, shot)
        else:
            logger.log('error', "Solution generation failed")
            json_logger.log_compilation_error(problem_data["title"], problem_data.get("category", "Uncategorized"), "No solution generated", "Solution generation failed", problems_passed, shot)

        if os.path.exists(source_file):
            os.remove(source_file)
        if os.path.exists(binary_file):
            os.remove(binary_file)
    
    return problems_passed

def main():
    logger = Logger()
    config = load_config('config.json')

    os.makedirs("benchmark", exist_ok=True)
    os.makedirs("temp", exist_ok=True)

    log_filename = os.path.join("benchmark", f"{sanitize_filename(config.provider)}_{sanitize_filename(config.model)}_log.json")
    
    if not config.continue_from_log:
        if os.path.exists(log_filename):
            os.remove(log_filename)
        json_logger = JSONLogger(log_filename)
        json_logger.log_initial_config(config)
    else:
        json_logger = JSONLogger(log_filename)
    
    problems = load_problems_from_hf("juvi21/cses-fi-competitive-coding-problems")
    
    categories_filter = config.categories  
    shots = config.shots
    ignore_time_limits = config.ignore_time_limits
    
    if config.language == "cpp":
        judge = CppJudge(logger)
    elif config.language == "python":
        judge = PythonJudge(logger)
    else:
        logger.log('error', "Unsupported language specified")
        raise ValueError("Unsupported language specified")
        
    provider = initialize_provider(config, logger)

    if categories_filter:
        filtered_problems = [problem for problem in problems if json.loads(problem).get("category") in categories_filter]
    else:
        filtered_problems = problems

    total_filtered_problems = len(filtered_problems)
    problems_passed = json_logger.data.get("total_passed_problems", 0)
    processed_titles = set(problem["title"] for problem in json_logger.data.get("problems", []))

    for index, problem_str in enumerate(tqdm(filtered_problems, desc="Processing problems")):
        problem_data = json.loads(problem_str)
        problem_title = problem_data['title']
        
        if problem_title in processed_titles:
            logger.log('info', f"Skipping already processed problem: {problem_title}")
            continue

        logger.log('info', f"Judging problem: {problem_title}")
        problems_passed = process_problem(judge, provider, problem_data, shots, ignore_time_limits, json_logger, logger, problems_passed, total_filtered_problems, index)

if __name__ == "__main__":
    main()


